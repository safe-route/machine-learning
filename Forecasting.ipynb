{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840aa704-6ca0-4fe9-af7a-e0c1d0e889ff",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d371c76a-f6f5-4d71-82f4-df7888d8107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Dataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d61ec-65bf-4370-aef3-b443dacf8410",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b989cef0-7d2b-4442-b155-e829007420ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "LEARNING_RATE = 5e-2\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "LOSS = tf.keras.losses.Huber()\n",
    "METRICS = [\"mae\", \"mse\"]\n",
    "EPOCHS = 1000\n",
    "\n",
    "# Data split\n",
    "TEST_SPLIT = 0.1                # Float or int\n",
    "VALID_SPLIT = 0.1               # Float\n",
    "TRAIN_SPLIT = 1 - VALID_SPLIT   # Float\n",
    "\n",
    "# Dataset window\n",
    "STEPS_SIZE = 15               # minutes\n",
    "PREDICTS_SIZE = 1             # minute(s)\n",
    "WINDOW_SIZE = STEPS_SIZE + PREDICTS_SIZE\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 64\n",
    "\n",
    "# Dataset frame\n",
    "NUM_OF_FEATURES = 6\n",
    "NUM_OF_LABELS = 2\n",
    "\n",
    "# File\n",
    "DATASET_FILE_1 = './dataset/user_1/usr_1_w1_y2022.csv'\n",
    "DATASET_FILE_2 = './dataset/user_1/usr_1_w2_y2022.csv'\n",
    "model_1_file = './model/user_1/'\n",
    "INFERENCE_DATASET_FILE = './dataset/user_1/usr_1_steps.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9518e-db34-40b1-b2f5-32640b80dde6",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec47759-8ee3-4f12-bfa7-213b5ccae868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118291.96053744997"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure distance\n",
    "def distance(coordinate_1:tuple[float,float], coordinate_2:tuple[float, float]) -> float:\n",
    "    \"\"\"Measure haversine distance between two coordinate.\n",
    "    \n",
    "    Note:\n",
    "    - coordinate : tuple of latitude and longitude, ex. (3.10326051, 91.23206407)\n",
    "    Reference: https://en.wikipedia.org/wiki/Haversine_formula\n",
    "    \"\"\"\n",
    "    # constants\n",
    "    earth_radius = 6371000 # in meters\n",
    "    \n",
    "    # unpack and convert params to radian\n",
    "    lat_1, long_1 = np.radians(coordinate_1)\n",
    "    lat_2, long_2 = np.radians(coordinate_2)\n",
    "    \n",
    "    d_lat = lat_2 - lat_1\n",
    "    d_long = long_2 - long_1\n",
    "    \n",
    "    # calculate and return distance\n",
    "    return 2 * earth_radius * np.arcsin (np.sqrt(\n",
    "        np.sin(d_lat/2) ** 2\n",
    "        + np.cos(lat_1) * np.cos(lat_2) * np.sin(d_long/2) ** 2))\n",
    "    \n",
    "distance((-6.200000, 106.816666),(-6.914744, 107.609810))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c65a6-31c7-4a6a-acaf-d061b6a43b73",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b66c0d",
   "metadata": {},
   "source": [
    "# BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "credentials = service_account.Credentials.from_service_account_file('./dataset/safe-route-351803-701f86f6b63e.json')\n",
    "\n",
    "project_id = 'safe-route-351803'\n",
    "bqclient = bigquery.Client(credentials= credentials,project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec11536",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = bqclient.query(\"\"\"\n",
    "   SELECT (FORMAT_DATE(\"%m/%d/%Y\", datetime)), time, lat, long\n",
    "   FROM user_data.user_1_week_1\n",
    "   LIMIT 10000\"\"\")\n",
    "\n",
    "#results = query_job.to_dataframe() # Wait for the job to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ee007-c846-4910-919e-24935b1f5c34",
   "metadata": {},
   "source": [
    "## Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82949590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataset(source):\n",
    "    \"\"\"Return a pandas dataframe object from given source\"\"\"\n",
    "    x = source.to_dataframe()\n",
    "    x.rename(columns = {'f0_':'date','lat':'latitude','long':'longitude'}, inplace = True)\n",
    "    x[\"time\"] = x[\"time\"].astype(\"string\", errors='ignore')\n",
    "    return x\n",
    "\n",
    "data = fetch_dataset(query_job)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b43bc0c-4536-4c9a-91e1-57d17bd259ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3/7/2022</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3/7/2022</td>\n",
       "      <td>0:02:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3/7/2022</td>\n",
       "      <td>0:04:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3/7/2022</td>\n",
       "      <td>0:06:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3/7/2022</td>\n",
       "      <td>0:08:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5035</th>\n",
       "      <td>3/13/2022</td>\n",
       "      <td>23:50:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>3/13/2022</td>\n",
       "      <td>23:52:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037</th>\n",
       "      <td>3/13/2022</td>\n",
       "      <td>23:54:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>3/13/2022</td>\n",
       "      <td>23:56:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5039</th>\n",
       "      <td>3/13/2022</td>\n",
       "      <td>23:58:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5040 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date      time  latitude   longitude\n",
       "0      3/7/2022   0:00:00 -6.268917  106.780112\n",
       "1      3/7/2022   0:02:00 -6.268917  106.780112\n",
       "2      3/7/2022   0:04:00 -6.268917  106.780112\n",
       "3      3/7/2022   0:06:00 -6.268917  106.780112\n",
       "4      3/7/2022   0:08:00 -6.268917  106.780112\n",
       "...         ...       ...       ...         ...\n",
       "5035  3/13/2022  23:50:00 -6.268917  106.779552\n",
       "5036  3/13/2022  23:52:00 -6.268917  106.779552\n",
       "5037  3/13/2022  23:54:00 -6.268917  106.779552\n",
       "5038  3/13/2022  23:56:00 -6.268917  106.779552\n",
       "5039  3/13/2022  23:58:00 -6.268917  106.779552\n",
       "\n",
       "[5040 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_dataset2(source):\n",
    "    \"\"\"Return a pandas dataframe object from given source\"\"\"\n",
    "    return pd.read_csv(source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903605fd-f4c3-4fdb-a30c-06fbee1ce433",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd045ce0-b9c8-4e74-974d-a1be0ea325a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5035</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1430</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1432</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1434</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1436</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5039</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1438</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5040 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  month  day_of_week  time  latitude   longitude\n",
       "0     2022      3            0     0 -6.268917  106.780112\n",
       "1     2022      3            0     2 -6.268917  106.780112\n",
       "2     2022      3            0     4 -6.268917  106.780112\n",
       "3     2022      3            0     6 -6.268917  106.780112\n",
       "4     2022      3            0     8 -6.268917  106.780112\n",
       "...    ...    ...          ...   ...       ...         ...\n",
       "5035  2022      3            6  1430 -6.268917  106.779552\n",
       "5036  2022      3            6  1432 -6.268917  106.779552\n",
       "5037  2022      3            6  1434 -6.268917  106.779552\n",
       "5038  2022      3            6  1436 -6.268917  106.779552\n",
       "5039  2022      3            6  1438 -6.268917  106.779552\n",
       "\n",
       "[5040 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_dataset(data):\n",
    "    \"\"\"Preprocess dataset by doing:\n",
    "    1. convert date to multiple column\n",
    "    2. convert time to cumulative minute\n",
    "    3. rearrange fields\"\"\"\n",
    "    # Converting date string to datetime\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "    data[\"day_of_week\"] = data[\"date\"].dt.day_of_week\n",
    "    data[\"month\"] = data[\"date\"].dt.month\n",
    "    data[\"year\"] = data[\"date\"].dt.year\n",
    "\n",
    "    # Converting time to cumulative minute\n",
    "    # source: https://stackoverflow.com/questions/17951820/convert-hhmmss-to-minutes-using-python-pandas\n",
    "    # credit: Andy Hayden\n",
    "    data[\"time\"] = data[\"time\"].str.split(':').apply(lambda time: int(time[0]) * 60 + int(time[1]))\n",
    "\n",
    "    # Removing unused column\n",
    "    del data[\"date\"]\n",
    "\n",
    "    # Rearrange column\n",
    "    data = data[[\"year\", \"month\", \"day_of_week\", \"time\", \"latitude\", \"longitude\"]]\n",
    "    return data\n",
    "\n",
    "data = preprocess_dataset(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae016e8-6c0d-42ee-9878-32f824a28aab",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d18e7ac1-d994-4d93-bd4c-db17e1232d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape\n",
      "Train : (4082, 6)\n",
      "Valid : (454, 6)\n",
      "Test  : (504, 6)\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(data, train_split, test_split):\n",
    "    \"\"\"Split data to train, valid, and test data\"\"\"\n",
    "    # Split train_valid data and test data\n",
    "    test_len = test_split\n",
    "    if type(test_split)==float:\n",
    "        test_len = int(test_len * len(data))\n",
    "    train_val_data, test_data = data[:-test_len], data[-test_len:]\n",
    "    \n",
    "    # Split train data and valid data\n",
    "    train_len = int(len(train_val_data) * train_split)\n",
    "    train_data, valid_data = train_val_data[:train_len], train_val_data[train_len:]\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "train_data, valid_data, test_data = split_dataset(data, TRAIN_SPLIT, TEST_SPLIT)\n",
    "\n",
    "print(\"Dataset Shape\")\n",
    "print(f'Train : {train_data.shape}')\n",
    "print(f'Valid : {valid_data.shape}')\n",
    "print(f'Test  : {test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f741993-e151-4736-ba1e-42ace4653a6c",
   "metadata": {},
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbb8b63e-2c8c-4e4f-91aa-939b614ff3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  (32, 15, 6)\n",
      "y =  (32, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "def windowed_dataset(data, steps_size, predicts_size, batch_size, shuffle_buffer):\n",
    "    \"\"\"Create windowed dataset\"\"\"\n",
    "    # Converting to tfds\n",
    "    wds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    # Data shifting\n",
    "    wds = wds.window(steps_size+predicts_size, shift=predicts_size, drop_remainder=True)\n",
    "    \n",
    "    # Flatten windows\n",
    "    wds = wds.flat_map(lambda window : window.batch(steps_size+predicts_size))\n",
    "    \n",
    "    # Create window tuples\n",
    "    wds = wds.map(lambda window: (window[:-predicts_size], window[-predicts_size:, -NUM_OF_LABELS:]))\n",
    "    \n",
    "    # Shuffle windows\n",
    "    wds = wds.shuffle(shuffle_buffer)\n",
    "    \n",
    "    # Batch windows\n",
    "    wds = wds.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return wds\n",
    "\n",
    "wds = windowed_dataset(data, STEPS_SIZE, PREDICTS_SIZE, BATCH_SIZE, SHUFFLE_BUFFER_SIZE)\n",
    "for idx,(x,y) in enumerate(wds):\n",
    "    print(\"x = \", x.numpy().shape)\n",
    "    print(\"y = \", y.numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d94a9-cac5-4d7d-84a9-438977bf02e0",
   "metadata": {},
   "source": [
    "## Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2750e720-05f9-4875-9bcb-c5d040a5afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(source, **kwargs):\n",
    "    \"\"\"Build dataset to make a train-ready dataset\n",
    "    list of valid kwargs:\n",
    "    - test_split: float - split value for test from whole dataset\n",
    "    - valid_split: float - split value for valid from train_valid dataset\n",
    "    - steps_size: int - number of steps used for prediction\n",
    "    - predicts_size: int - number of predictions\n",
    "    - batch_size: int - dataset batch size\n",
    "    - shuffle_buffer_size: int - shuffle buffer size\n",
    "    - num_of_features: int - number of features\n",
    "    - num_of_labels: int - number of labels\n",
    "    \"\"\"\n",
    "    # BUILD CONSTANTS\n",
    "    # Data split\n",
    "    test_split = kwargs.get('test_split', TEST_SPLIT)\n",
    "    valid_split = kwargs.get('valid_split', VALID_SPLIT)\n",
    "    train_split = 1 - valid_split\n",
    "\n",
    "    # Dataset window\n",
    "    steps_size = kwargs.get('steps_size', STEPS_SIZE)\n",
    "    predicts_size = kwargs.get('predicts_size', PREDICTS_SIZE)\n",
    "    window_size = steps_size + predicts_size\n",
    "    batch_size = kwargs.get('batch_size', BATCH_SIZE)\n",
    "    shuffle_buffer_size =  kwargs.get('shuffle_buffer_size', SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "    # Dataset frame\n",
    "    num_of_features = kwargs.get('num_of_features', NUM_OF_FEATURES)\n",
    "    num_of_labels = kwargs.get('num_of_labels',NUM_OF_LABELS)\n",
    "    \n",
    "    # FETCHING DATASET\n",
    "    ds = fetch_dataset(source) # use await for later asynchrounous usage\n",
    "    \n",
    "    # PREPROCESSING DATASET\n",
    "    ds = preprocess_dataset(ds)\n",
    "    \n",
    "    # SPLITTING DATASET\n",
    "    _train_ds, _valid_ds, _test_ds = split_dataset(ds, train_split, test_split)\n",
    "    \n",
    "    # WINDOWING AND RETURNING DATASET\n",
    "    return \\\n",
    "        windowed_dataset(_train_ds, steps_size, predicts_size, batch_size, shuffle_buffer_size), \\\n",
    "        windowed_dataset(_valid_ds, steps_size, predicts_size, batch_size, shuffle_buffer_size), \\\n",
    "        windowed_dataset(_test_ds, steps_size, predicts_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "train_wds, valid_wds, test_wds = build_dataset(query_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3737d5-7226-4a3e-85a7-0c42c0147134",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5b562-71ae-41af-b98d-f1e495480172",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72f7ab93-954b-4f7b-af29-5de1697b7ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 15, 64)            18176     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 15, 32)            12416     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 16)                3136      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,154\n",
      "Trainable params: 34,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    \"\"\"Create Forecasting Model\n",
    "    Model used: LSTM\n",
    "    output should consist of 2 item, latitude and longitude\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Generating model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(64, activation='sigmoid', input_shape=(STEPS_SIZE, NUM_OF_FEATURES), return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32, activation='sigmoid', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(16, activation='sigmoid'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(16, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(8, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(NUM_OF_LABELS, activation='linear')\n",
    "    ])\n",
    "\n",
    "    # Compiling model\n",
    "    model.compile(\n",
    "        loss=LOSS,\n",
    "        optimizer=OPTIMIZER,\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc392864-8bf9-4537-b844-d553821aac28",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371563be-a495-4827-8b85-b96fafb969e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "510/510 [==============================] - 12s 16ms/step - loss: 14.3103 - mae: 14.4850 - mse: 1054.3308 - val_loss: 5.6166e-04 - val_mae: 0.0252 - val_mse: 0.0011\n",
      "Epoch 2/20\n",
      "510/510 [==============================] - 8s 15ms/step - loss: 2.2290e-04 - mae: 0.0136 - mse: 4.4579e-04 - val_loss: 5.2971e-04 - val_mae: 0.0236 - val_mse: 0.0011\n",
      "Epoch 3/20\n",
      "510/510 [==============================] - 10s 19ms/step - loss: 1.9105e-04 - mae: 0.0127 - mse: 3.8209e-04 - val_loss: 4.8999e-04 - val_mae: 0.0214 - val_mse: 9.8239e-04\n",
      "Epoch 4/20\n",
      "510/510 [==============================] - 10s 19ms/step - loss: 1.7836e-04 - mae: 0.0128 - mse: 3.5674e-04 - val_loss: 6.9862e-04 - val_mae: 0.0300 - val_mse: 0.0014\n",
      "Epoch 5/20\n",
      "510/510 [==============================] - 9s 17ms/step - loss: 1.3959e-04 - mae: 0.0112 - mse: 2.7923e-04 - val_loss: 6.0258e-04 - val_mae: 0.0254 - val_mse: 0.0012\n",
      "Epoch 6/20\n",
      "510/510 [==============================] - 9s 17ms/step - loss: 1.2698e-04 - mae: 0.0104 - mse: 2.5399e-04 - val_loss: 6.3570e-04 - val_mae: 0.0254 - val_mse: 0.0013\n",
      "Epoch 7/20\n",
      "510/510 [==============================] - 9s 17ms/step - loss: 1.3209e-04 - mae: 0.0101 - mse: 2.6421e-04 - val_loss: 7.5467e-04 - val_mae: 0.0312 - val_mse: 0.0015\n",
      "Epoch 8/20\n",
      "510/510 [==============================] - 9s 17ms/step - loss: 1.3473e-04 - mae: 0.0102 - mse: 2.6943e-04 - val_loss: 6.4678e-04 - val_mae: 0.0278 - val_mse: 0.0013\n",
      "Epoch 9/20\n",
      "510/510 [==============================] - 9s 17ms/step - loss: 1.3479e-04 - mae: 0.0106 - mse: 2.6955e-04 - val_loss: 5.9681e-04 - val_mae: 0.0234 - val_mse: 0.0012\n",
      "Epoch 10/20\n",
      "510/510 [==============================] - 9s 18ms/step - loss: 1.1863e-04 - mae: 0.0101 - mse: 2.3721e-04 - val_loss: 5.1402e-04 - val_mae: 0.0228 - val_mse: 0.0010\n",
      "Epoch 11/20\n",
      "510/510 [==============================] - 9s 17ms/step - loss: 1.3311e-04 - mae: 0.0105 - mse: 2.6624e-04 - val_loss: 5.5100e-04 - val_mae: 0.0227 - val_mse: 0.0011\n",
      "Epoch 12/20\n",
      "510/510 [==============================] - 9s 18ms/step - loss: 1.5631e-04 - mae: 0.0108 - mse: 3.1261e-04 - val_loss: 4.9421e-04 - val_mae: 0.0219 - val_mse: 9.9087e-04\n",
      "Epoch 13/20\n",
      "510/510 [==============================] - 9s 18ms/step - loss: 1.3017e-04 - mae: 0.0097 - mse: 2.6032e-04 - val_loss: 5.3952e-04 - val_mae: 0.0226 - val_mse: 0.0011\n",
      "Epoch 14/20\n",
      "510/510 [==============================] - 9s 18ms/step - loss: 1.2608e-04 - mae: 0.0099 - mse: 2.5220e-04 - val_loss: 4.9713e-04 - val_mae: 0.0221 - val_mse: 9.9717e-04\n",
      "Epoch 15/20\n",
      "510/510 [==============================] - 9s 18ms/step - loss: 1.2794e-04 - mae: 0.0102 - mse: 2.5589e-04 - val_loss: 6.0110e-04 - val_mae: 0.0256 - val_mse: 0.0012\n",
      "Epoch 16/20\n",
      "510/510 [==============================] - 9s 18ms/step - loss: 1.5470e-04 - mae: 0.0118 - mse: 3.0943e-04 - val_loss: 4.8815e-04 - val_mae: 0.0213 - val_mse: 9.7892e-04\n",
      "Epoch 17/20\n",
      "510/510 [==============================] - 10s 19ms/step - loss: 1.5068e-04 - mae: 0.0116 - mse: 3.0133e-04 - val_loss: 5.1490e-04 - val_mae: 0.0225 - val_mse: 0.0010\n",
      "Epoch 18/20\n",
      "510/510 [==============================] - 10s 20ms/step - loss: 1.7168e-04 - mae: 0.0125 - mse: 3.4330e-04 - val_loss: 6.9811e-04 - val_mae: 0.0299 - val_mse: 0.0014\n",
      "Epoch 19/20\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 1.8737e-04 - mae: 0.0126 - mse: 3.7478e-04 - val_loss: 6.1466e-04 - val_mae: 0.0261 - val_mse: 0.0012\n",
      "Epoch 20/20\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 1.4047e-04 - mae: 0.0116 - mse: 2.8095e-04 - val_loss: 5.9942e-04 - val_mae: 0.0256 - val_mse: 0.0012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16b77ca3f70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_wds, epochs=20, validation_data=valid_wds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76cb9a6-3289-49fd-908f-1766da1f383d",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8dd2f1-26a3-4bbd-9950-54455af14fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_wds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f684fc8-ec1d-4857-966f-4ec348c2da01",
   "metadata": {},
   "source": [
    "## Using Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6cff7-8512-4fe4-a188-a4ee4178a943",
   "metadata": {},
   "source": [
    "### Converting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd3560-e268-48a4-a8f5-963dc55064ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data):\n",
    "    \"\"\"Convert data to model input\"\"\"\n",
    "    # Take last \"steps size\" data from copy data\n",
    "    cdata = data.copy()[-STEPS_SIZE:]\n",
    "    if len(cdata) != STEPS_SIZE:\n",
    "        # Not enough data to do prediction\n",
    "        return None\n",
    "    # Add empty row in the bottom\n",
    "    cdata.loc[cdata.shape[0]] = np.zeros(NUM_OF_FEATURES)\n",
    "    \n",
    "    return windowed_dataset(cdata, STEPS_SIZE, PREDICTS_SIZE, BATCH_SIZE, SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "predict_data = build_inference_dataset(INFERENCE_DATASET_FILE)\n",
    "predict_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973a9d4-a4c4-4f7b-9827-9d4e26c951d5",
   "metadata": {},
   "source": [
    "### Build Inference Input (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc171132-f6a9-4201-9948-abae0bf120a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 6), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 2), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_inference_dataset(source, **kwargs):\n",
    "    \"\"\"Build inference dataset in the form of prefetch dataset from given source\"\"\"\n",
    "    # BUILD CONSTANTS\n",
    "    # Data split\n",
    "    test_split = kwargs.get('test_split', TEST_SPLIT)\n",
    "    valid_split = kwargs.get('valid_split', VALID_SPLIT)\n",
    "    train_split = 1 - valid_split\n",
    "\n",
    "    # Dataset window\n",
    "    steps_size = kwargs.get('steps_size', STEPS_SIZE)\n",
    "    predicts_size = kwargs.get('predicts_size', PREDICTS_SIZE)\n",
    "    window_size = steps_size + predicts_size\n",
    "    batch_size = kwargs.get('batch_size', BATCH_SIZE)\n",
    "    shuffle_buffer_size =  kwargs.get('shuffle_buffer_size', SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "    # Dataset frame\n",
    "    num_of_features = kwargs.get('num_of_features', NUM_OF_FEATURES)\n",
    "    num_of_labels = kwargs.get('num_of_labels',NUM_OF_LABELS)\n",
    "    \n",
    "    # FETCHING DATASET\n",
    "    ds = fetch_dataset2(source) # use await for later asynchrounous usage\n",
    "    ds = ds[-steps_size:] # take only n-steps\n",
    "    \n",
    "    # PREPROCESSING DATASET\n",
    "    ds = preprocess_dataset(ds)\n",
    "    \n",
    "    # CONVERTING TO INFERENCE SHAPE\n",
    "    if len(ds) != steps_size:\n",
    "        return None\n",
    "    # Add empty row as empty label\n",
    "    ds.loc[ds.shape[0]] = np.zeros(num_of_features)\n",
    "    \n",
    "    # WINDOWING AND RETURNING DATASET\n",
    "    return windowed_dataset(ds, steps_size, predicts_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "predict_data = build_inference_dataset(INFERENCE_DATASET_FILE)\n",
    "predict_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e57bfa-f570-4ded-9315-bd4565effe0d",
   "metadata": {},
   "source": [
    "### Predicting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af3bbc2f-6ce9-47f9-9f14-03481e7b9b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "Predict\n",
      "========================================\n",
      "\n",
      "1/1 [==============================] - 1s 623ms/step\n",
      "[ -6.2280617 106.81565  ]\n",
      "\n",
      "\n",
      "========================================\n",
      "Label\n",
      "========================================\n",
      "\n",
      "[-6.229087622, 106.7979363]\n"
     ]
    }
   ],
   "source": [
    "def predict(model, data):\n",
    "    \"\"\"Predict data using model and data\"\"\"\n",
    "    steps_data = convert_data(data)\n",
    "    if not steps_data:\n",
    "        # If converting failed (not enough data) return none\n",
    "        return None\n",
    "    # Else return prediction\n",
    "    return  model.predict(\n",
    "        steps_data\n",
    "    )[0]\n",
    "\n",
    "\n",
    "print(\"\\n\\n========================================\")\n",
    "print(\"Predict\")\n",
    "print(\"========================================\\n\")\n",
    "print(predict(model, data[:6763]))\n",
    "\n",
    "print(\"\\n\\n========================================\")\n",
    "print(\"Label\")\n",
    "print(\"========================================\\n\")\n",
    "print([data.loc[6763][\"latitude\"], data.loc[6763][\"longitude\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6baaba7-08f3-4b00-acc2-e78960d797a5",
   "metadata": {},
   "source": [
    "## Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acc6c8db-1e01-463f-aceb-51a16498fe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "1961.1701042958343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants\n",
    "max_range = 1000 # in meters\n",
    "\n",
    "# Trigger function\n",
    "def calculate_trigger(\n",
    "        coordinate_1:tuple[float,float],\n",
    "        coordinate_2:tuple[float,float],\n",
    "        max_range:float) -> bool:\n",
    "    \"\"\"Calculate if distance between two coordinates is \n",
    "    over the max range and return True if distance is more\n",
    "    than max_range\"\"\"\n",
    "    print(distance(coordinate_1, coordinate_2))\n",
    "    return distance(coordinate_1, coordinate_2) > max_range\n",
    "\n",
    "calculate_trigger(\n",
    "    predict(model, data[:6763]),\n",
    "    (data.loc[6763][\"latitude\"], data.loc[6763][\"longitude\"]),\n",
    "    max_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267a771-a5eb-48e9-8665-143e0771148a",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4837386c-c607-421c-a0fb-36d8cb4911da",
   "metadata": {},
   "source": [
    "- [Sequences, Time Series and Prediction by DeepLearning.AI](https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction)\n",
    "- [Multi-Variate Time Series Forecasting Tensorflow by Nicholas Jhana](https://www.kaggle.com/code/nicholasjhana/multi-variate-time-series-forecasting-tensorflow/notebook#Visualizing-Predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
