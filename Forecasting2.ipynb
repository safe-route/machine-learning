{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840aa704-6ca0-4fe9-af7a-e0c1d0e889ff",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d371c76a-f6f5-4d71-82f4-df7888d8107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d61ec-65bf-4370-aef3-b443dacf8410",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b989cef0-7d2b-4442-b155-e829007420ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "LEARNING_RATE = 5e-2\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "LOSS = tf.keras.losses.Huber()\n",
    "METRICS = [\"mae\", \"mse\"]\n",
    "EPOCHS = 1000\n",
    "\n",
    "# Data split\n",
    "TEST_SPLIT = 0.1                # Float or int\n",
    "VALID_SPLIT = 0.1               # Float\n",
    "TRAIN_SPLIT = 1 - VALID_SPLIT   # Float\n",
    "\n",
    "# Dataset window\n",
    "STEPS_SIZE = 15               # minutes\n",
    "PREDICTS_SIZE = 1             # minute(s)\n",
    "WINDOW_SIZE = STEPS_SIZE + PREDICTS_SIZE\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 64\n",
    "\n",
    "# Dataset frame\n",
    "NUM_OF_FEATURES = 6\n",
    "NUM_OF_LABELS = 2\n",
    "\n",
    "# File\n",
    "DATASET_FILE_1 = './dataset/user_1/usr_1_w1_y2022.csv'\n",
    "DATASET_FILE_2 = './dataset/user_1/usr_1_w2_y2022.csv'\n",
    "model_1_file = './model/user_1/'\n",
    "INFERENCE_DATASET_FILE = './dataset/user_1/a.json'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "with open (INFERENCE_DATASET_FILE) as f:\n",
    "  data = json.load(f)\n",
    "INFERENCE_DATASET_FILE2 = json.dumps(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9518e-db34-40b1-b2f5-32640b80dde6",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fec47759-8ee3-4f12-bfa7-213b5ccae868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118291.96053744997"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure distance\n",
    "def distance(coordinate_1:tuple[float,float], coordinate_2:tuple[float, float]) -> float:\n",
    "    \"\"\"Measure haversine distance between two coordinate.\n",
    "    \n",
    "    Note:\n",
    "    - coordinate : tuple of latitude and longitude, ex. (3.10326051, 91.23206407)\n",
    "    Reference: https://en.wikipedia.org/wiki/Haversine_formula\n",
    "    \"\"\"\n",
    "    # constants\n",
    "    earth_radius = 6371000 # in meters\n",
    "    \n",
    "    # unpack and convert params to radian\n",
    "    lat_1, long_1 = np.radians(coordinate_1)\n",
    "    lat_2, long_2 = np.radians(coordinate_2)\n",
    "    \n",
    "    d_lat = lat_2 - lat_1\n",
    "    d_long = long_2 - long_1\n",
    "    \n",
    "    # calculate and return distance\n",
    "    return 2 * earth_radius * np.arcsin (np.sqrt(\n",
    "        np.sin(d_lat/2) ** 2\n",
    "        + np.cos(lat_1) * np.cos(lat_2) * np.sin(d_long/2) ** 2))\n",
    "    \n",
    "distance((-6.200000, 106.816666),(-6.914744, 107.609810))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c65a6-31c7-4a6a-acaf-d061b6a43b73",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ee007-c846-4910-919e-24935b1f5c34",
   "metadata": {},
   "source": [
    "## Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b43bc0c-4536-4c9a-91e1-57d17bd259ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3/7/2022</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3/7/2022</td>\n",
       "      <td>0:02:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3/7/2022</td>\n",
       "      <td>0:04:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3/7/2022</td>\n",
       "      <td>0:06:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3/7/2022</td>\n",
       "      <td>0:08:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5035</th>\n",
       "      <td>3/13/2022</td>\n",
       "      <td>23:50:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>3/13/2022</td>\n",
       "      <td>23:52:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037</th>\n",
       "      <td>3/13/2022</td>\n",
       "      <td>23:54:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>3/13/2022</td>\n",
       "      <td>23:56:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5039</th>\n",
       "      <td>3/13/2022</td>\n",
       "      <td>23:58:00</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5040 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date      time  latitude   longitude\n",
       "0      3/7/2022   0:00:00 -6.268917  106.780112\n",
       "1      3/7/2022   0:02:00 -6.268917  106.780112\n",
       "2      3/7/2022   0:04:00 -6.268917  106.780112\n",
       "3      3/7/2022   0:06:00 -6.268917  106.780112\n",
       "4      3/7/2022   0:08:00 -6.268917  106.780112\n",
       "...         ...       ...       ...         ...\n",
       "5035  3/13/2022  23:50:00 -6.268917  106.779552\n",
       "5036  3/13/2022  23:52:00 -6.268917  106.779552\n",
       "5037  3/13/2022  23:54:00 -6.268917  106.779552\n",
       "5038  3/13/2022  23:56:00 -6.268917  106.779552\n",
       "5039  3/13/2022  23:58:00 -6.268917  106.779552\n",
       "\n",
       "[5040 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_dataset(source):\n",
    "    if  source[-1] == 'v':\n",
    "        data = pd.read_csv(source)\n",
    "    else :\n",
    "        meta = json.loads(source)\n",
    "        data = pd.DataFrame(meta)\n",
    "    return data\n",
    "\n",
    "data = fetch_dataset(DATASET_FILE_1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fdf552f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_io.TextIOWrapper' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19988/1690676181.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINFERENCE_DATASET_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19988/920156366.py\u001b[0m in \u001b[0;36mfetch_dataset\u001b[1;34m(source)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfetch_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mif\u001b[0m  \u001b[0msource\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'v'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '_io.TextIOWrapper' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "f = open(INFERENCE_DATASET_FILE)\n",
    "x = fetch_dataset(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903605fd-f4c3-4fdb-a30c-06fbee1ce433",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd045ce0-b9c8-4e74-974d-a1be0ea325a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.780112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5035</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1430</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1432</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1434</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1436</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5039</th>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1438</td>\n",
       "      <td>-6.268917</td>\n",
       "      <td>106.779552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5040 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  month  day_of_week  time  latitude   longitude\n",
       "0     2022      3            0     0 -6.268917  106.780112\n",
       "1     2022      3            0     2 -6.268917  106.780112\n",
       "2     2022      3            0     4 -6.268917  106.780112\n",
       "3     2022      3            0     6 -6.268917  106.780112\n",
       "4     2022      3            0     8 -6.268917  106.780112\n",
       "...    ...    ...          ...   ...       ...         ...\n",
       "5035  2022      3            6  1430 -6.268917  106.779552\n",
       "5036  2022      3            6  1432 -6.268917  106.779552\n",
       "5037  2022      3            6  1434 -6.268917  106.779552\n",
       "5038  2022      3            6  1436 -6.268917  106.779552\n",
       "5039  2022      3            6  1438 -6.268917  106.779552\n",
       "\n",
       "[5040 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_dataset(data):\n",
    "    \"\"\"Preprocess dataset by doing:\n",
    "    1. convert date to multiple column\n",
    "    2. convert time to cumulative minute\n",
    "    3. rearrange fields\"\"\"\n",
    "    # Converting date string to datetime\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "    data[\"day_of_week\"] = data[\"date\"].dt.day_of_week\n",
    "    data[\"month\"] = data[\"date\"].dt.month\n",
    "    data[\"year\"] = data[\"date\"].dt.year\n",
    "\n",
    "    # Converting time to cumulative minute\n",
    "    # source: https://stackoverflow.com/questions/17951820/convert-hhmmss-to-minutes-using-python-pandas\n",
    "    # credit: Andy Hayden\n",
    "    data[\"time\"] = data[\"time\"].str.split(':').apply(lambda time: int(time[0]) * 60 + int(time[1]))\n",
    "\n",
    "    # Removing unused column\n",
    "    del data[\"date\"]\n",
    "\n",
    "    # Rearrange column\n",
    "    data = data[[\"year\", \"month\", \"day_of_week\", \"time\", \"latitude\", \"longitude\"]]\n",
    "    return data\n",
    "\n",
    "data = preprocess_dataset(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae016e8-6c0d-42ee-9878-32f824a28aab",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d18e7ac1-d994-4d93-bd4c-db17e1232d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape\n",
      "Train : (4082, 6)\n",
      "Valid : (454, 6)\n",
      "Test  : (504, 6)\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(data, train_split, test_split):\n",
    "    \"\"\"Split data to train, valid, and test data\"\"\"\n",
    "    # Split train_valid data and test data\n",
    "    test_len = test_split\n",
    "    if type(test_split)==float:\n",
    "        test_len = int(test_len * len(data))\n",
    "    train_val_data, test_data = data[:-test_len], data[-test_len:]\n",
    "    \n",
    "    # Split train data and valid data\n",
    "    train_len = int(len(train_val_data) * train_split)\n",
    "    train_data, valid_data = train_val_data[:train_len], train_val_data[train_len:]\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "train_data, valid_data, test_data = split_dataset(data, TRAIN_SPLIT, TEST_SPLIT)\n",
    "\n",
    "print(\"Dataset Shape\")\n",
    "print(f'Train : {train_data.shape}')\n",
    "print(f'Valid : {valid_data.shape}')\n",
    "print(f'Test  : {test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f741993-e151-4736-ba1e-42ace4653a6c",
   "metadata": {},
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb8b63e-2c8c-4e4f-91aa-939b614ff3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  (32, 15, 6)\n",
      "y =  (32, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "def windowed_dataset(data, steps_size, predicts_size, batch_size, shuffle_buffer):\n",
    "    \"\"\"Create windowed dataset\"\"\"\n",
    "    # Converting to tfds\n",
    "    wds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    # Data shifting\n",
    "    wds = wds.window(steps_size+predicts_size, shift=predicts_size, drop_remainder=True)\n",
    "    \n",
    "    # Flatten windows\n",
    "    wds = wds.flat_map(lambda window : window.batch(steps_size+predicts_size))\n",
    "    \n",
    "    # Create window tuples\n",
    "    wds = wds.map(lambda window: (window[:-predicts_size], window[-predicts_size:, -NUM_OF_LABELS:]))\n",
    "    \n",
    "    # Shuffle windows\n",
    "    wds = wds.shuffle(shuffle_buffer)\n",
    "    \n",
    "    # Batch windows\n",
    "    wds = wds.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return wds\n",
    "\n",
    "wds = windowed_dataset(data, STEPS_SIZE, PREDICTS_SIZE, BATCH_SIZE, SHUFFLE_BUFFER_SIZE)\n",
    "for idx,(x,y) in enumerate(wds):\n",
    "    print(\"x = \", x.numpy().shape)\n",
    "    print(\"y = \", y.numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d94a9-cac5-4d7d-84a9-438977bf02e0",
   "metadata": {},
   "source": [
    "## Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2750e720-05f9-4875-9bcb-c5d040a5afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(source, **kwargs):\n",
    "    \"\"\"Build dataset to make a train-ready dataset\n",
    "    list of valid kwargs:\n",
    "    - test_split: float - split value for test from whole dataset\n",
    "    - valid_split: float - split value for valid from train_valid dataset\n",
    "    - steps_size: int - number of steps used for prediction\n",
    "    - predicts_size: int - number of predictions\n",
    "    - batch_size: int - dataset batch size\n",
    "    - shuffle_buffer_size: int - shuffle buffer size\n",
    "    - num_of_features: int - number of features\n",
    "    - num_of_labels: int - number of labels\n",
    "    \"\"\"\n",
    "    # BUILD CONSTANTS\n",
    "    # Data split\n",
    "    test_split = kwargs.get('test_split', TEST_SPLIT)\n",
    "    valid_split = kwargs.get('valid_split', VALID_SPLIT)\n",
    "    train_split = 1 - valid_split\n",
    "\n",
    "    # Dataset window\n",
    "    steps_size = kwargs.get('steps_size', STEPS_SIZE)\n",
    "    predicts_size = kwargs.get('predicts_size', PREDICTS_SIZE)\n",
    "    window_size = steps_size + predicts_size\n",
    "    batch_size = kwargs.get('batch_size', BATCH_SIZE)\n",
    "    shuffle_buffer_size =  kwargs.get('shuffle_buffer_size', SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "    # Dataset frame\n",
    "    num_of_features = kwargs.get('num_of_features', NUM_OF_FEATURES)\n",
    "    num_of_labels = kwargs.get('num_of_labels',NUM_OF_LABELS)\n",
    "    \n",
    "    # FETCHING DATASET\n",
    "    ds = fetch_dataset(source) # use await for later asynchrounous usage\n",
    "    \n",
    "    # PREPROCESSING DATASET\n",
    "    ds = preprocess_dataset(ds)\n",
    "    \n",
    "    # SPLITTING DATASET\n",
    "    _train_ds, _valid_ds, _test_ds = split_dataset(ds, train_split, test_split)\n",
    "    \n",
    "    # WINDOWING AND RETURNING DATASET\n",
    "    return \\\n",
    "        windowed_dataset(_train_ds, steps_size, predicts_size, batch_size, shuffle_buffer_size), \\\n",
    "        windowed_dataset(_valid_ds, steps_size, predicts_size, batch_size, shuffle_buffer_size), \\\n",
    "        windowed_dataset(_test_ds, steps_size, predicts_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "train_wds, valid_wds, test_wds = build_dataset(DATASET_FILE_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3737d5-7226-4a3e-85a7-0c42c0147134",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5b562-71ae-41af-b98d-f1e495480172",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72f7ab93-954b-4f7b-af29-5de1697b7ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 15, 64)            18176     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 15, 32)            12416     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 16)                3136      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,154\n",
      "Trainable params: 34,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    \"\"\"Create Forecasting Model\n",
    "    Model used: LSTM\n",
    "    output should consist of 2 item, latitude and longitude\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Generating model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(64, activation='sigmoid', input_shape=(STEPS_SIZE, NUM_OF_FEATURES), return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32, activation='sigmoid', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(16, activation='sigmoid'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(16, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(8, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(NUM_OF_LABELS, activation='linear')\n",
    "    ])\n",
    "\n",
    "    # Compiling model\n",
    "    model.compile(\n",
    "        loss=LOSS,\n",
    "        optimizer=OPTIMIZER,\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc392864-8bf9-4537-b844-d553821aac28",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "371563be-a495-4827-8b85-b96fafb969e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "128/128 [==============================] - 6s 23ms/step - loss: 47.0927 - mae: 47.4660 - mse: 4439.8306 - val_loss: 39.7400 - val_mae: 40.0193 - val_mse: 3198.2400\n",
      "Epoch 2/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 33.1852 - mae: 33.4589 - mse: 2265.1064 - val_loss: 26.4950 - val_mae: 26.7840 - val_mse: 1430.4080\n",
      "Epoch 3/20\n",
      "128/128 [==============================] - 2s 17ms/step - loss: 20.0276 - mae: 20.3044 - mse: 851.7085 - val_loss: 12.2990 - val_mae: 12.5829 - val_mse: 314.8916\n",
      "Epoch 4/20\n",
      "128/128 [==============================] - 2s 17ms/step - loss: 3.3625 - mae: 3.5641 - mse: 57.4533 - val_loss: 0.0010 - val_mae: 0.0388 - val_mse: 0.0020\n",
      "Epoch 5/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 4.0684e-04 - mae: 0.0208 - mse: 8.1081e-04 - val_loss: 0.0012 - val_mae: 0.0367 - val_mse: 0.0024\n",
      "Epoch 6/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 3.4069e-04 - mae: 0.0192 - mse: 6.8032e-04 - val_loss: 0.0014 - val_mae: 0.0400 - val_mse: 0.0028\n",
      "Epoch 7/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 3.8092e-04 - mae: 0.0199 - mse: 7.5922e-04 - val_loss: 0.0011 - val_mae: 0.0355 - val_mse: 0.0022\n",
      "Epoch 8/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 4.1929e-04 - mae: 0.0209 - mse: 8.3479e-04 - val_loss: 9.5897e-04 - val_mae: 0.0342 - val_mse: 0.0019\n",
      "Epoch 9/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 4.5964e-04 - mae: 0.0221 - mse: 9.1447e-04 - val_loss: 1.5023e-04 - val_mae: 0.0140 - val_mse: 3.0174e-04\n",
      "Epoch 10/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 5.0586e-04 - mae: 0.0231 - mse: 0.0010 - val_loss: 6.4088e-04 - val_mae: 0.0302 - val_mse: 0.0013\n",
      "Epoch 11/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 4.8712e-04 - mae: 0.0226 - mse: 9.6857e-04 - val_loss: 3.5551e-04 - val_mae: 0.0224 - val_mse: 7.1393e-04\n",
      "Epoch 12/20\n",
      "128/128 [==============================] - 2s 19ms/step - loss: 5.1154e-04 - mae: 0.0236 - mse: 0.0010 - val_loss: 4.9847e-04 - val_mae: 0.0281 - val_mse: 9.9871e-04\n",
      "Epoch 13/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 4.7630e-04 - mae: 0.0231 - mse: 9.5146e-04 - val_loss: 3.8108e-04 - val_mae: 0.0256 - val_mse: 7.5987e-04\n",
      "Epoch 14/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 5.3715e-04 - mae: 0.0242 - mse: 0.0011 - val_loss: 4.9273e-04 - val_mae: 0.0275 - val_mse: 9.8263e-04\n",
      "Epoch 15/20\n",
      "128/128 [==============================] - 2s 19ms/step - loss: 5.3451e-04 - mae: 0.0242 - mse: 0.0011 - val_loss: 3.4203e-04 - val_mae: 0.0212 - val_mse: 6.8307e-04\n",
      "Epoch 16/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 4.7360e-04 - mae: 0.0224 - mse: 9.4681e-04 - val_loss: 4.1421e-04 - val_mae: 0.0224 - val_mse: 8.2498e-04\n",
      "Epoch 17/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 4.8015e-04 - mae: 0.0221 - mse: 9.5891e-04 - val_loss: 8.4076e-04 - val_mae: 0.0315 - val_mse: 0.0017\n",
      "Epoch 18/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 4.9733e-04 - mae: 0.0219 - mse: 9.9460e-04 - val_loss: 0.0014 - val_mae: 0.0393 - val_mse: 0.0027\n",
      "Epoch 19/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 5.0264e-04 - mae: 0.0217 - mse: 0.0010 - val_loss: 9.7148e-04 - val_mae: 0.0324 - val_mse: 0.0019\n",
      "Epoch 20/20\n",
      "128/128 [==============================] - 2s 18ms/step - loss: 4.6031e-04 - mae: 0.0203 - mse: 9.2107e-04 - val_loss: 0.0012 - val_mae: 0.0365 - val_mse: 0.0025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2038c2a17c0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_wds, epochs=20, validation_data=valid_wds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76cb9a6-3289-49fd-908f-1766da1f383d",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f8dd2f1-26a3-4bbd-9950-54455af14fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0023 - mae: 0.0513 - mse: 0.0045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0022662500850856304, 0.051327671855688095, 0.004546389915049076]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_wds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f684fc8-ec1d-4857-966f-4ec348c2da01",
   "metadata": {},
   "source": [
    "## Using Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6cff7-8512-4fe4-a188-a4ee4178a943",
   "metadata": {},
   "source": [
    "### Converting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dbd3560-e268-48a4-a8f5-963dc55064ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 6), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 2), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_data(data):\n",
    "    \"\"\"Convert data to model input\"\"\"\n",
    "    # Take last \"steps size\" data from copy data\n",
    "    cdata = data.copy()[-STEPS_SIZE:]\n",
    "    if len(cdata) != STEPS_SIZE:\n",
    "        # Not enough data to do prediction\n",
    "        return None\n",
    "    # Add empty row in the bottom\n",
    "    cdata.loc[cdata.shape[0]] = np.zeros(NUM_OF_FEATURES)\n",
    "    \n",
    "    return windowed_dataset(cdata, STEPS_SIZE, PREDICTS_SIZE, BATCH_SIZE, SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "predict_data = build_inference_dataset(INFERENCE_DATASET_FILE)\n",
    "predict_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973a9d4-a4c4-4f7b-9827-9d4e26c951d5",
   "metadata": {},
   "source": [
    "### Build Inference Input (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc171132-f6a9-4201-9948-abae0bf120a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 6), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 2), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_inference_dataset(source, **kwargs):\n",
    "    \"\"\"Build inference dataset in the form of prefetch dataset from given source\"\"\"\n",
    "    # BUILD CONSTANTS\n",
    "    # Data split\n",
    "    test_split = kwargs.get('test_split', TEST_SPLIT)\n",
    "    valid_split = kwargs.get('valid_split', VALID_SPLIT)\n",
    "    train_split = 1 - valid_split\n",
    "\n",
    "    # Dataset window\n",
    "    steps_size = kwargs.get('steps_size', STEPS_SIZE)\n",
    "    predicts_size = kwargs.get('predicts_size', PREDICTS_SIZE)\n",
    "    window_size = steps_size + predicts_size\n",
    "    batch_size = kwargs.get('batch_size', BATCH_SIZE)\n",
    "    shuffle_buffer_size =  kwargs.get('shuffle_buffer_size', SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "    # Dataset frame\n",
    "    num_of_features = kwargs.get('num_of_features', NUM_OF_FEATURES)\n",
    "    num_of_labels = kwargs.get('num_of_labels',NUM_OF_LABELS)\n",
    "    \n",
    "    # FETCHING DATASET\n",
    "    ds = fetch_dataset(source) # use await for later asynchrounous usage\n",
    "    ds = ds[-steps_size:] # take only n-steps\n",
    "    \n",
    "    # PREPROCESSING DATASET\n",
    "    ds = preprocess_dataset(ds)\n",
    "    \n",
    "    # CONVERTING TO INFERENCE SHAPE\n",
    "    if len(ds) != steps_size:\n",
    "        return None\n",
    "    # Add empty row as empty label\n",
    "    ds.loc[ds.shape[0]] = np.zeros(num_of_features)\n",
    "    \n",
    "    # WINDOWING AND RETURNING DATASET\n",
    "    return windowed_dataset(ds, steps_size, predicts_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "predict_data = build_inference_dataset(INFERENCE_DATASET_FILE2)\n",
    "predict_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e57bfa-f570-4ded-9315-bd4565effe0d",
   "metadata": {},
   "source": [
    "### Predicting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af3bbc2f-6ce9-47f9-9f14-03481e7b9b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "Predict\n",
      "========================================\n",
      "\n",
      "1/1 [==============================] - 0s 374ms/step\n",
      "[ -6.1883955 106.77954  ]\n",
      "\n",
      "\n",
      "========================================\n",
      "Label\n",
      "========================================\n",
      "\n",
      "[-6.268917097, 106.7795517]\n"
     ]
    }
   ],
   "source": [
    "def predict(model, data):\n",
    "    \"\"\"Predict data using model and data\"\"\"\n",
    "    steps_data = convert_data(data)\n",
    "    if not steps_data:\n",
    "        # If converting failed (not enough data) return none\n",
    "        return None\n",
    "    # Else return prediction\n",
    "    return  model.predict(\n",
    "        steps_data\n",
    "    )[0]\n",
    "\n",
    "\n",
    "print(\"\\n\\n========================================\")\n",
    "print(\"Predict\")\n",
    "print(\"========================================\\n\")\n",
    "print(predict(model, data[:5000]))\n",
    "\n",
    "print(\"\\n\\n========================================\")\n",
    "print(\"Label\")\n",
    "print(\"========================================\\n\")\n",
    "print([data.loc[5000][\"latitude\"], data.loc[5000][\"longitude\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6baaba7-08f3-4b00-acc2-e78960d797a5",
   "metadata": {},
   "source": [
    "## Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acc6c8db-1e01-463f-aceb-51a16498fe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "8953.585736508117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants\n",
    "max_range = 1000 # in meters\n",
    "\n",
    "# Trigger function\n",
    "def calculate_trigger(\n",
    "        coordinate_1:tuple[float,float],\n",
    "        coordinate_2:tuple[float,float],\n",
    "        max_range:float) -> bool:\n",
    "    \"\"\"Calculate if distance between two coordinates is \n",
    "    over the max range and return True if distance is more\n",
    "    than max_range\"\"\"\n",
    "    print(distance(coordinate_1, coordinate_2))\n",
    "    return distance(coordinate_1, coordinate_2) > max_range\n",
    "\n",
    "calculate_trigger(\n",
    "    predict(model, data[:5000]),\n",
    "    (data.loc[5000][\"latitude\"], data.loc[5000][\"longitude\"]),\n",
    "    max_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267a771-a5eb-48e9-8665-143e0771148a",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4837386c-c607-421c-a0fb-36d8cb4911da",
   "metadata": {},
   "source": [
    "- [Sequences, Time Series and Prediction by DeepLearning.AI](https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction)\n",
    "- [Multi-Variate Time Series Forecasting Tensorflow by Nicholas Jhana](https://www.kaggle.com/code/nicholasjhana/multi-variate-time-series-forecasting-tensorflow/notebook#Visualizing-Predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
